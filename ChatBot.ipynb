{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import keras\n",
    "import os\n",
    "from pandas import Series\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Embedding\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel('QA.1.xlsx')\n",
    "data2 = pd.read_excel('QA.2.xlsx')\n",
    "data3 = pd.read_excel('QA.3.xlsx')\n",
    "data4 = pd.read_excel('QA_4.xlsx')\n",
    "data5 = pd.read_excel('QA_5.xlsx')\n",
    "data6 = pd.read_excel('QA_6.xlsx')\n",
    "data = pd.concat([data1, data2, data3, data4, data5, data6])\n",
    "#data.to_csv('Compiled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(data4['Answer'])[0]\n",
    "#list(data4['Answer'])[0].replace('\\n',' ').split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data1.head()\n",
    "# data2.head()\n",
    "# data3.head()\n",
    "# data4.head()\n",
    "# data5.head()\n",
    "# data6.head()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.tokenize.sent_tokenize((data[data['Question'] == 'What is meant by selection bias?']['Answer'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.tokenize.sent_tokenize(list(data['Answer'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = (data.set_index(['Question'])\n",
    "         .apply(lambda x: x.str.replace('\\n',' ').str.split('.').explode())\n",
    "         .reset_index())\n",
    "sdata['Answer'].replace('', np.nan, inplace = True)\n",
    "csdata = sdata.dropna().reset_index(drop = True)\n",
    "df = csdata.loc[csdata['Answer'].str.len() > 3].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_root(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word.lower(), pos = 'n') for word in words]\n",
    "    return lemmatized\n",
    "def preprocess(pr):\n",
    "    #Remove Punctuations\n",
    "    no_punc = ''.join([character for character in pr if character not in string.punctuation])  \n",
    "    #Tokenize the PRT\n",
    "    tokenized = nltk.word_tokenize(no_punc.lower()) \n",
    "    #Remove Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    no_stop = [word.lower() for word in tokenized if word.lower() not in stop_words]\n",
    "    no_numeric = [word for word in no_stop if not any(num.isdigit() for num in word)]\n",
    "    #Generating the root words of the tokens\n",
    "    root_word = generate_root(no_numeric)        \n",
    "    return root_word\n",
    "\n",
    "def preprocessQ(pr):\n",
    "    #Remove Punctuations from Q only\n",
    "    no_punc = ''.join([character for character in pr if character not in string.punctuation])\n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_puncs(text):\n",
    "    no_punc = ''.join([character for character in text if character not in string.punctuation])\n",
    "    return no_punc\n",
    "df['CleanA'] = df['Answer'].apply(lambda x: remove_puncs(x))\n",
    "#df['CleanQ'] = df['Question'].apply(lambda x: remove_puncs(x))\n",
    "\n",
    "def tokenize(pr):\n",
    "    tokenized = nltk.word_tokenize(pr.lower())\n",
    "    return tokenized\n",
    "df['TokenizedA'] = df['CleanA'].apply(lambda x: tokenize(x.lower()))\n",
    "#df['TokenizedQ'] = df['CleanQ'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "def remove_stop(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    no_stop = [word.lower() for word in text if word.lower() not in stop_words]\n",
    "    no_numeric = [word for word in no_stop if not any(num.isdigit() for num in word)] #We may want to keep numbers\n",
    "    #return no_stop\n",
    "    return no_numeric\n",
    "df['No_StopA'] = df['TokenizedA'].apply(lambda x: remove_stop(x))\n",
    "# df['No_StopQ'] = df['TokenizedQ'].apply(lambda x: remove_stop(x))\n",
    "\n",
    "\n",
    "def generate_root(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word.lower(), pos = 'n') for word in words]\n",
    "    return lemmatized\n",
    "def rooting(sentence):\n",
    "    root_word = generate_root(sentence)        \n",
    "    return root_word\n",
    "df['RootedA'] = df['No_StopA'].apply(lambda x: rooting(x))\n",
    "df['FilteredA'] = df['RootedA'].apply(lambda x: (' ').join(x))\n",
    "# df['RootedQ'] = df['No_StopQ'].apply(lambda x: rooting(x))\n",
    "# df['FilteredQ'] = df['RootedQ'].apply(lambda x: (' ').join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finaldf = df[['Question', 'Filtered']]\n",
    "# finaldf = finaldf.loc[finaldf['Filtered'].str.len() > 3].reset_index(drop = True)\n",
    "finaldf = df[['Question', 'FilteredA']]\n",
    "finaldf = finaldf.loc[finaldf['FilteredA'].str.len() > 3].reset_index(drop = True)\n",
    "pairs = list(zip(finaldf.Question, finaldf.FilteredA))\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medium's Sample Guide\n",
    "input_docs = list(finaldf['FilteredQ']) #List of all Questions\n",
    "target_docs = [] #list(finaldf['Filtered']) #List of all Filtered Answers\n",
    "input_tokens = set() #Unique Vocabulary in Questions\n",
    "target_tokens = set() #Unique Vocabulary in Answers\n",
    "for line in pairs:\n",
    "    input_doc, target_doc = line[0], line[1]\n",
    "    # Splitting words from punctuation  \n",
    "    target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "    # Redefine target_doc below and append it to target_docs\n",
    "    target_doc = '<START> ' + target_doc + ' <END>'\n",
    "    target_docs.append(target_doc)\n",
    "\n",
    "    # Now we split up each sentence into words and add each unique word to our vocabulary set\n",
    "    for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "        if token not in input_tokens:\n",
    "            input_tokens.add(token)\n",
    "    for token in target_doc.split():\n",
    "        if token not in target_tokens:\n",
    "            target_tokens.add(token)\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "input_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(input_tokens)])\n",
    "target_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "reverse_input_features_dict = dict(\n",
    "    (i, token) for token, i in input_features_dict.items())\n",
    "reverse_target_features_dict = dict(\n",
    "    (i, token) for token, i in target_features_dict.items())\n",
    "\n",
    "#Maximum length of sentences in input and target documents\n",
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
    "encoder_input_data = np.zeros((len(input_docs), max_encoder_seq_length, num_encoder_tokens), dtype = 'float32')\n",
    "decoder_input_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens), dtype = 'float32')\n",
    "decoder_target_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens), dtype = 'float32')\n",
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "    for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "        #Assign 1. for the current line, timestep, & word in encoder_input_data\n",
    "        encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "    for timestep, token in enumerate(target_doc.split()):\n",
    "        decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "        if timestep > 0:\n",
    "            decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_text = list(finaldf['Question'])\n",
    "def addtags(decoder_input_sentence):\n",
    "    bt = \"<BOS> \"\n",
    "    et = \" <EOS>\"\n",
    "    final_target = [bt + text + et for text in decoder_input_sentence] \n",
    "    return final_target\n",
    "decoder_input_text = addtags(finaldf['FilteredA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_creater(text_lists, VOCAB_SIZE):\n",
    "    tokenizer = Tokenizer(num_words = VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    dictionary = tokenizer.word_index\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k, v in dictionary.items():\n",
    "        if v < VOCAB_SIZE:\n",
    "            word2idx[k] = v\n",
    "            idx2word[v] = k\n",
    "        if v >= VOCAB_SIZE-1:\n",
    "            continue      \n",
    "    return word2idx, idx2word\n",
    "word2idx, idx2word = vocab_creater(encoder_input_text + decoder_input_text, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eword2idx, eidx2word = vocab_creater(encoder_input_text, 10000)\n",
    "num_unique_encoder = len(eword2idx)\n",
    "dword2idx, didx2word = vocab_creater(decoder_input_text, 10000)\n",
    "num_unique_decoder = len(dword2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2seq(encoder_text, decoder_text, VOCAB_SIZE):\n",
    "    tokenizer = Tokenizer(num_words = VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(encoder_text + decoder_text)\n",
    "    encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
    "    decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
    "    return encoder_sequences, decoder_sequences\n",
    "\n",
    "encoder_sequences, decoder_sequences = text2seq(encoder_input_text, decoder_input_text, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", inputs)) for inputs in encoder_input_text])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", inputs)) for inputs in decoder_input_text])\n",
    "if max_encoder_seq_length > max_decoder_seq_length:\n",
    "    MAX_LEN = max_encoder_seq_length\n",
    "else:\n",
    "    MAX_LEN = max_decoder_seq_length\n",
    "    \n",
    "def padding(encoder_sequences, decoder_sequences, MAX_LEN):\n",
    "    encoder_input_data = pad_sequences(encoder_sequences, maxlen = max_encoder_seq_length, dtype = 'int32', padding = 'post', truncating = 'post')\n",
    "    decoder_input_data = pad_sequences(decoder_sequences, maxlen = max_decoder_seq_length, dtype = 'int32', padding = 'post', truncating = 'post')\n",
    "    return encoder_input_data, decoder_input_data\n",
    "encoder_input_data, decoder_input_data = padding(encoder_sequences, decoder_sequences, MAX_LEN)\n",
    "decoder_target_data = np.zeros((len(encoder_input_text), max_decoder_seq_length), dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = max(encoder_input_text, key = len) \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_layer = Input(shape = (max_encoder_seq_length, ))\n",
    "decoder_input_layer = Input(shape = (max_decoder_seq_length, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_100d_dictionary(): #Creating the embedding index with GloVe's 100d.\n",
    "    embeddings_index = {}\n",
    "    data = open('glove.6B.100d.txt', encoding = 'utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    data.close()\n",
    "    return embeddings_index\n",
    "embeddings_index = glove_100d_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix_creater(embedding_dimension): #Creating the embedding matrix with the embedding index.\n",
    "    embedding_matrix = np.zeros((len(word2idx) + 1, embedding_dimension))\n",
    "    for word, i in word2idx.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "embedding_matrix = embedding_matrix_creater(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix): #Creating the embedding layer.\n",
    "    embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                                output_dim = EMBEDDING_DIM,\n",
    "                                input_length = MAX_LEN,\n",
    "                                weights = [embedding_matrix],\n",
    "                                trainable = False)\n",
    "    return embedding_layer\n",
    "embedding_layer = embedding_layer_creater(3041, 100, MAX_LEN, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_spliter(encoder_input_data, decoder_input_data, test_size1 = 0.2, test_size2 = 0.3):\n",
    "    encoder_train, encoder_test, decoder_train, decoder_test = train_test_split(encoder_input_data, decoder_input_data, test_size = test_size1)\n",
    "    encoder_train, encoder_val, decoder_train, decoder_val = train_test_split(encoder_train, decoder_train, test_size = test_size2)\n",
    "    return encoder_train, encoder_val, encoder_test, decoder_train, decoder_val, decoder_test\n",
    "encoder_train, encoder_val, encoder_test, decoder_train, decoder_val, decoder_test = data_spliter(encoder_input_data, decoder_input_data)\n",
    "\n",
    "print(encoder_train.shape)\n",
    "print(encoder_val.shape)\n",
    "print(encoder_test.shape)\n",
    "print(decoder_train.shape)\n",
    "print(decoder_val.shape)\n",
    "print(decoder_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_inputs = Input(shape = (MAX_LEN, ), dtype = 'int32',)\n",
    "encoder_inputs = Input(shape=(num_unique_encoder, ), dtype = 'int32',)\n",
    "#encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
    "#encoder_embedding = embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(256, return_state = True)\n",
    "#encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_inputs)\n",
    "\n",
    "#decoder_inputs = Input(shape = (MAX_LEN, ), dtype = 'int32',)\n",
    "decoder_inputs = Input(shape = (num_unique_decoder, ), dtype = 'int32',)\n",
    "#decoder_inputs = Input(shape = (None, num_decoder_tokens))\n",
    "#decoder_embedding = embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(256, return_state = True, return_sequences = True)\n",
    "#dense_layer = Dense(VOCAB_SIZE, activation = 'softmax')\n",
    "#decoder_dense_layer = Dense(num_unique_decoder, activation = 'relu')\n",
    "decoder_dense_layer = Dense(num_unique_decoder, activation = 'softmax')\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state = [state_h, state_c])\n",
    "#decoder_outputs = decoder_dense_layer(decoder_outputs)\n",
    "outputs = TimeDistributed(Dense(10000, activation = 'softmax'))(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "#Compiling\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'], sample_weight_mode = 'temporal')\n",
    "#model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'], sample_weight_mode = 'temporal')\n",
    "\n",
    "#Fitting\n",
    "model.fit([encoder_train, decoder_train], outputs, batch_size = 64, epochs = 60, shuffle = True)\n",
    "#model.fit([encoder_train, decoder_train], outputs, batch_size = 32, epochs = 60, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_response(test_input):\n",
    "    states_value = encoder_model.predict(test_input)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_features_dict['<BOS>']] = 1.\n",
    "    decoded_sentence = ''\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if (sampled_token == '<EOS>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "            target_seq[0, 0, sampled_token_index] = 1.\n",
    "            states_value = [hidden_state, cell_state]\n",
    "    return decoded_sentence\n",
    "class ans:\n",
    "    def string_to_matrix(self, user_input):\n",
    "        tokens = preprocessQ(user_input)\n",
    "        user_input_matrix = np.zeros(\n",
    "          (1, max_encoder_seq_length, num_encoder_tokens), dtype = 'float32')\n",
    "        for timestep, token in enumerate(tokens):\n",
    "            if token in input_features_dict:\n",
    "                user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n",
    "        return user_input_matrix\n",
    "    def generate_response(self, user_input):\n",
    "        input_matrix = self.string_to_matrix(user_input)\n",
    "        chatbot_response = decode_response(input_matrix)\n",
    "        chatbot_response = chatbot_response.replace(\"<BOS>\",'')\n",
    "        chatbot_response = chatbot_response.replace(\"<EOS>\",'')\n",
    "        return chatbot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questiontoask = 'What is meant by selection bias?'\n",
    "answertoquestion = ans().generate_response(Questiontoask)\n",
    "print('Input: ' + Questiontoask)\n",
    "print('Output: ' + answertoquestion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
